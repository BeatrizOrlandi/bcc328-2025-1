% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\author{Construção de compiladores I}
\date{}
\title{Análise Léxica}
\hypersetup{
 pdfauthor={Construção de compiladores I},
 pdftitle={Análise Léxica},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 29.4 (Org mode 9.7.22)}, 
 pdflang={English}}
\begin{document}

\maketitle
\section*{Objetivos}
\label{sec:org148f440}

\subsection*{Objetivos}
\label{sec:org36b3f89}

\begin{itemize}
\item Apresentar a importância da etapa de análise léxica
em um compilador.
\item Apresentar a implementação de um analisador léxico ad-hoc
para uma linguagem simples.
\end{itemize}
\subsection*{Objetivos}
\label{sec:org35b4d25}

\begin{itemize}
\item Mostrar como a teoria de expressões regulares e autômatos pode ser utilizada para automatizar a criação de analisadores léxicos.
\end{itemize}
\subsection*{Objetivos}
\label{sec:orge444581}

\begin{itemize}
\item Apresentar o gerador de analisadore léxicos, Alex
\end{itemize}
\section*{Análise léxica}
\label{sec:org2b127f2}

\subsection*{Análise léxica}
\label{sec:orgd403b79}

\begin{itemize}
\item Primeira etapa do front-end de um compilador.
\item Simplificar a entrada para análise sintática.
\end{itemize}
\subsection*{Análise léxica}
\label{sec:org9154b0e}

\begin{itemize}
\item Simplificações:
\begin{itemize}
\item Remoção de espaços em branco.
\item Remoção de comentários.
\end{itemize}

\item Resultado: lista de \textbf{\textbf{tokens}}.
\end{itemize}
\subsection*{Análise léxica}
\label{sec:orgbdef0fe}

\begin{itemize}
\item Token
\begin{itemize}
\item Componente indivisível da sintaxe de uma linguagem.
\end{itemize}
\end{itemize}
\subsection*{Análise léxica}
\label{sec:org31b7d2a}

\begin{itemize}
\item Exemplos de tokens:
\begin{itemize}
\item identificadores
\item palavras reservadas
\item separadores
\item literais
\end{itemize}
\end{itemize}
\subsection*{Análise léxica}
\label{sec:org69ba591}

\begin{itemize}
\item Como implementar a análise léxica?
\end{itemize}
\subsection*{Análise léxica ad-hoc}
\label{sec:org9cd9b28}

\begin{itemize}
\item Percorra a string:
\begin{itemize}
\item Se for um dígito, guarde-o para formar um número.
\item Se for um operador, gere o token.
\item Se for um parêntesis, gere o token.
\item Se for um espaço, tente gerar um número e descarte o espaço.
\end{itemize}
\end{itemize}
\subsection*{Análise léxica ad-hoc}
\label{sec:org65d199b}

\begin{itemize}
\item Como representar tokens?
\end{itemize}

\begin{verbatim}
data Token
  = Number Int
  | Add
  | Minus
  | LParen
  | RParen
  deriving (Eq, Show)
\end{verbatim}
\subsection*{Análise léxica ad-hoc}
\label{sec:org291d2b6}

\begin{itemize}
\item Configuração do analisador léxico
\begin{itemize}
\item Lista de tokens encontrados.
\item String de dígitos consecutivos encontrados.
\end{itemize}
\end{itemize}

\begin{verbatim}
type LexerState = Maybe ([Token], String)
\end{verbatim}
\subsection*{Análise léxica ad-hoc}
\label{sec:org708e85f}

\begin{itemize}
\item Como finalizar um número?
\begin{itemize}
\item Encontrando um espaço ou operador, criamos um token
com os dígitos.
\end{itemize}
\end{itemize}

\begin{verbatim}
updateState :: LexerState -> LexerState
updateState Nothing = Nothing
updateState ac@(Just (ts, ns))
  | all isDigit ns && not (null ns)
    = let v = read (reverse ns)
      in Just (Number v : ts, [])
  | otherwise =ac
\end{verbatim}
\subsection*{Análise léxica ad-hoc}
\label{sec:orga710ae1}

\begin{itemize}
\item Iterando sobre a string de entrada.
\end{itemize}

\begin{verbatim}
lexer' :: LexerState -> String -> LexerState
lexer' ac [] = updateState ac
lexer' Nothing _ = Nothing
lexer' ac@(Just (ts, ns)) (c:cs)
  | isSpace c = lexer' (updateState ac) cs
  | isDigit c = lexer' (Just (ts, c : ns)) cs
  | c == '(' = lexer' (Just (LParen : ts, ns)) cs
  | c == ')' = lexer' (Just (RParen : ts, ns)) cs
  | c == '+' = lexer' (Just (Add : ts, ns)) cs
  | c == '*' = lexer' (Just (Mult : ts, ns)) cs
  | otherwise = Nothing
\end{verbatim}
\subsection*{Análise léxica ad-hoc}
\label{sec:orgbac4e07}

\begin{itemize}
\item Interface principal do analisador
\end{itemize}

\begin{verbatim}
lexer :: String -> Maybe [Token]
lexer s
  = case lexer' (Just ([], "")) s of
      Nothing -> Nothing
      Just (ts, _) -> Just (reverse ts)
\end{verbatim}
\subsection*{Análise léxica ad-hoc}
\label{sec:org5510db1}

\begin{itemize}
\item Algoritmo simples para análise léxica de uma linguagem.

\item Problema: difícil de extender.
\begin{itemize}
\item Como incluir números de ponto flutuante?
\item Como incluir identificadores e palavras reservadas?
\end{itemize}
\end{itemize}
\subsection*{Análise léxica ad-hoc}
\label{sec:org4532257}

\begin{itemize}
\item Para acomodar essas mudanças, precisamos de uma abordagem
sistemática para a análise léxica.

\item Para isso, utilizaremos a teoria de expressões regulares
e autômatos finitos.
\end{itemize}
\section*{Expressões regulares}
\label{sec:orgcf96b02}

\subsection*{Expressões regulares}
\label{sec:org5342412}

\begin{itemize}
\item Formalismo algébrico para descrição de linguagens.
\item Amplamente utilizado para representação de padrões em texto.
\item Análise léxica: dividir texto em subpadrões de interesse.
\end{itemize}
\subsection*{Expressões regulares}
\label{sec:org5894ee8}

\begin{itemize}
\item Qual a relação entre ERs e análise léxica?
\begin{itemize}
\item Usando ERs podemos \textbf{\textbf{automatizar}} a construção de analisadores léxicos.
\end{itemize}
\end{itemize}
\subsection*{Expressões regulares}
\label{sec:org392ef48}

\begin{itemize}
\item Em essência, um analisador léxico é um AFD que produz
uma lista de tokens.
\item Em Teoria da computação, vimos que toda ER possui um AFD
equivalente
\begin{itemize}
\item Construção de Thompson / derivadas
\end{itemize}
\end{itemize}
\subsection*{Expressões regulares}
\label{sec:org5c45d86}

\begin{itemize}
\item Construção de Thompson
\begin{itemize}
\item Baseada em propriedades de fechamento de AFs.
\item Cria um AFN com transições lambda.
\end{itemize}
\end{itemize}
\subsection*{Expressões regulares}
\label{sec:org0ac13a4}

\begin{itemize}
\item Construção de Thompson para lambda.
\end{itemize}

\begin{center}
\includegraphics[width=.9\linewidth]{./Thompson-epsilon.png}
\end{center}
\subsection*{Expressões regulares}
\label{sec:org62a6674}

\begin{itemize}
\item Construção de Thompson para símbolo.
\end{itemize}

\begin{center}
\includegraphics[width=.9\linewidth]{./Thompson-a-symbol.png}
\end{center}
\subsection*{Expressões regulares}
\label{sec:orgb7c03b1}

\begin{itemize}
\item Construção de Thompson para união.
\end{itemize}
\begin{center}
\includegraphics[width=.9\linewidth]{./Thompson-or.png}
\end{center}
\subsection*{Expressões regulares}
\label{sec:org7c46dc8}

\begin{itemize}
\item Construção de Thompson para concatenação.
\end{itemize}

\begin{center}
\includegraphics[width=.9\linewidth]{./Thompson-concat.png}
\end{center}
\subsection*{Expressões regulares}
\label{sec:org3fdd3ca}

\begin{itemize}
\item Construção de Thompson para Kleene.
\end{itemize}

\begin{center}
\includegraphics[width=.9\linewidth]{./Thompson-kleene-star.png}
\end{center}
\subsection*{Expressões regulares}
\label{sec:org8850c02}

\begin{itemize}
\item Como representar AFD em código?
\begin{itemize}
\item Normalmente, utilizamos uma matriz para representar a função de transição.
\end{itemize}
\end{itemize}
\subsection*{Expressões regulares}
\label{sec:org93704b2}

\begin{itemize}
\item Representando um AFD:
\end{itemize}

\begin{verbatim}
-- a: type for states
-- b: type for alphabet symbols

data DFA a b
  = DFA {
      start :: a
    , trans :: [((a,b), a)]
    , final :: [a]
    } deriving Show
\end{verbatim}
\subsection*{Expressões regulares}
\label{sec:org453d3cf}

\begin{itemize}
\item Processando palavras usando o AFD
\end{itemize}

\begin{verbatim}
delta :: (Eq a, Eq b) => DFA a b -> [b] -> Maybe a
delta m s = foldl step (Just (start m)) s
  where
    step (Just e) a
      = lookup (e,a) (trans m)
    step Nothing _ = Nothing
\end{verbatim}
\subsection*{Expressões regulares}
\label{sec:orgca3d048}

\begin{itemize}
\item Representando o AFD de números:
\end{itemize}

\begin{verbatim}
data State = S0 | S1 deriving (Eq, Show)

numberDFA :: DFA State Char
numberDFA
  = DFA {
      start = S0
    , trans = [((S0, c), S1) | c <- ['0'..'9']] ++
              [((S1, c), S1) | c <- ['0'..'9']]
    , final = [S1]
    }
\end{verbatim}
\subsection*{Expressões regulares}
\label{sec:orgdfea454}

\begin{itemize}
\item Como usar AFDs para produzir os tokens?
\begin{itemize}
\item Crie o token usando o maior prefixo possível processado.
\end{itemize}
\end{itemize}
\subsection*{Expressões regulares}
\label{sec:org8faf851}

\begin{itemize}
\item Produzindo um token
\end{itemize}

\begin{verbatim}
extract :: DFA State Char -> String -> (String, String)
extract m s = go (start m) "" s
  where
    go _ token [] = (token, [])
    go e token (x : xs)
      | isSpace x = (token, x : xs)
      | otherwise = case lookup (e,x) (trans m) of
                      Just e' -> go e' (token ++ [x]) xs
                      Nothing -> (token, x : xs)
\end{verbatim}
\subsection*{Expressões regulares}
\label{sec:org2595a70}

\begin{itemize}
\item Analisador léxico
\end{itemize}

\begin{verbatim}
dfaLexer :: DFA State Char -> String -> [Token]
dfaLexer m s = go s []
  where
    go [] ac = reverse ac
    go (x : xs) ac
      | isSpace x = go xs ac
      | otherwise =
        let (token, rest) = extract m (x : xs)
        in go rest (if null token then ac else Number (read token) : ac)
\end{verbatim}
\subsection*{Expressões regulares}
\label{sec:org1dbf551}

\begin{itemize}
\item Esse código simples funciona para apenas um AFD.

\item A especificação de uma linguagem é formada por várias ERs.
\begin{itemize}
\item Como combiná-las para produzir um AFD?
\end{itemize}
\end{itemize}
\subsection*{Expressões regulares}
\label{sec:orgf01cbf7}

\begin{itemize}
\item Como combinar AFDs?
\begin{itemize}
\item Propriedades de fechamento!
\end{itemize}

\item Processo automatizável utilizando geradores de analisadores léxicos.
\end{itemize}
\section*{Analisadores léxicos}
\label{sec:orgb2fb122}

\subsection*{Analisadores léxicos}
\label{sec:org688d9bd}

\begin{itemize}
\item Geradores de analisadores produzem a representação de AFDs mínimos
a partir de uma especificação descrita como expressões regulares.

\item Abordagens baseadas no teorema de Kleene / derivadas
\end{itemize}
\subsection*{Analisadores léxicos}
\label{sec:org494a8d2}

\begin{itemize}
\item Para Haskell, podemos utilizar a ferramenta \href{https://github.com/haskell/alex}{Alex.}

\item Produz o código Haskell correspondente ao analisador léxico a partir de
uma especificação.
\end{itemize}
\subsection*{Analisadores léxicos}
\label{sec:orga787be9}

\begin{itemize}
\item Componentes de uma especificação Alex.
\begin{itemize}
\item Código Haskell
\item Especificação de expressões regulares.
\item Definição de \emph{wrapper}.
\end{itemize}
\end{itemize}
\subsection*{Analisadores léxicos}
\label{sec:orgd91f4fc}

\begin{itemize}
\item Trechos de código Haskell
\begin{itemize}
\item Definem funções utilizadas para criação de tokens
\item Definir o tipo do token
\item Definição de módulo e importações.
\end{itemize}
\end{itemize}
\subsection*{Analisadores léxicos}
\label{sec:org800f629}

\begin{itemize}
\item Expressões regulares.
\end{itemize}

\begin{verbatim}
$digit = 0-9
@number = $digit+

tokens :-
      -- whitespace and comments
      <0> $white+       ;
      <0> "--" .*       ;
      -- other tokens
      <0> @number       {mkNumber}
      <0> "("           {simpleToken TLParen}
      <0> ")"           {simpleToken TRParen}
      <0> "+"           {simpleToken TPlus}
      <0> "*"           {simpleToken TTimes}
\end{verbatim}
\subsection*{Analisadores léxicos}
\label{sec:org40172ce}

\begin{itemize}
\item Expressões regulares.
\begin{itemize}
\item O ``.'' representa qualquer caractere diferente da quebra de linha.
\end{itemize}
\end{itemize}
\subsection*{Analisadores léxicos}
\label{sec:orgf5530f3}

\begin{itemize}
\item Cada token é formado por:
\begin{itemize}
\item Especificação do estado do analisador (<0>).
\item Expressão regular (@number).
\item Ação semântica executada quando do reconhecimento (mkNumber).
\end{itemize}
\end{itemize}
\subsection*{Analisadores léxicos}
\label{sec:org378d40d}

\begin{itemize}
\item Expressões regulares.
\begin{itemize}
\item macros usando \$: definem conjuntos de caracteres
\item macros usando @: definem expressões regulares.
\end{itemize}
\end{itemize}
\subsection*{Analisadores léxicos}
\label{sec:org6893010}

\begin{itemize}
\item Exemplo: macro de caractere
\end{itemize}

\begin{verbatim}
$digit = 0-9
\end{verbatim}
\subsection*{Analisadores léxicos}
\label{sec:org40b9ad2}

\begin{itemize}
\item Exemplo: macro de expressões regulares
\end{itemize}

\begin{verbatim}
@number = $digit+
\end{verbatim}
\subsection*{Analisadores léxicos}
\label{sec:org7860fc1}

\begin{itemize}
\item Especificando a criação de tokens
\end{itemize}

\begin{verbatim}
tokens :-
      -- whitespace and comments
      <0> $white+       ;
      -- other tokens
      <0> @number       {mkNumber}
      <0> "("           {simpleToken TLParen}
      <0> ")"           {simpleToken TRParen}
      <0> "+"           {simpleToken TPlus}
      <0> "*"           {simpleToken TTimes}
      <0> "-"           {simpleToken TMinus}
\end{verbatim}
\subsection*{Analisadores léxicos}
\label{sec:org7eb3f2d}

\begin{itemize}
\item Especificando a criação de tokens
\begin{itemize}
\item Para cada ER, apresentamos código para construir o token correspondente
\item Deve ter tipo \texttt{AlexInput -> Int64 -> Alex Token}
\end{itemize}
\item Tipo \texttt{AlexInput}
\end{itemize}

\begin{verbatim}
type AlexInput = (AlexPosn,    -- current position,
                  Char,        -- previous char
                  ByteString,  -- current input string
                  Int64)       -- bytes consumed so far
\end{verbatim}
\subsection*{Analisadores léxicos}
\label{sec:orga04cf25}

\begin{itemize}
\item Exemplo: criando token de número
\end{itemize}

\begin{verbatim}
mkNumber :: AlexAction Token
mkNumber (st, _, _, str) len
  = pure $ Token (position st) (TNumber $ read $ take len str)
\end{verbatim}
\subsection*{Analisadores léxicos}
\label{sec:orgcfcabdf}

\begin{itemize}
\item Exemplo: criando token de operadores e separadores
\end{itemize}

\begin{verbatim}
simpleToken :: Lexeme -> AlexAction Token
simpleToken lx (st, _, _, _) _
  = return $ Token (position st) lx
\end{verbatim}
\subsection*{Analisadores léxicos}
\label{sec:org067e3c6}

\begin{itemize}
\item Função top-level do analisador.
\end{itemize}

\begin{verbatim}
lexer :: String -> Either String [Token]
lexer s = runAlex s go
  where
    go = do
      output <- alexMonadScan
      if lexeme output == TEOF then
        pure [output]
      else (output :) <$> go
\end{verbatim}
\subsection*{Analisadores léxicos}
\label{sec:orgdcd8a59}

\begin{itemize}
\item Especificação de exemplo:
\begin{itemize}
\item pasta \texttt{Alex/LexerExample.x}
\end{itemize}
\end{itemize}
\subsection*{Analisadores léxicos}
\label{sec:orgfa9c21e}

\begin{itemize}
\item Produzindo o código Haskell do analisador.
\begin{itemize}
\item Construído automaticamente pelo \emph{cabal}.
\end{itemize}
\end{itemize}

\begin{verbatim}
alex LexerExample.x -o LexerExample.hs
\end{verbatim}
\subsection*{Analisadores léxicos}
\label{sec:org8fe7db9}

\begin{itemize}
\item Outros detalhes da especificação.
\begin{itemize}
\item wrapper do analisador.
\item definição do estado do analisador.
\item definição de outros estados e transições entre eles.
\end{itemize}
\end{itemize}
\subsection*{Analisadores léxicos}
\label{sec:orgc1bcafe}

\begin{itemize}
\item Wrapper do analisador: define o ``modelo'' de código a ser produzido pelo gerador Alex.
\begin{itemize}
\item No exemplo, usamos o mais geral dos templates.
\end{itemize}
\end{itemize}

\begin{verbatim}
%wrapper "monadUserState"
\end{verbatim}
\subsection*{Analisadores léxicos}
\label{sec:orge9e93b4}

\begin{itemize}
\item Definição do estado do analisador
\begin{itemize}
\item Qualquer tipo Haskell cujo nome deve ser \texttt{AlexUserState}.
\end{itemize}
\end{itemize}

\begin{verbatim}
data AlexUserState
  = AlexUserState {
       nestLevel :: Int -- comment nesting level
    }
\end{verbatim}
\subsection*{Analisadores léxicos}
\label{sec:org7b4400a}

\begin{itemize}
\item Estado inicial do analisador.
\begin{itemize}
\item Deve possui o nome \texttt{alexInitUserState} de tipo \texttt{AlexUserState}.
\end{itemize}
\end{itemize}

\begin{verbatim}
alexInitUserState :: AlexUserState
alexInitUserState
  = AlexUserState 0
\end{verbatim}
\subsection*{Analisadores léxicos}
\label{sec:org4aa3356}

\begin{itemize}
\item Interface para manipular o estado.
\end{itemize}

\begin{verbatim}
get :: Alex AlexUserState
get = Alex $ \s -> Right (s, alex_ust s)

put :: AlexUserState -> Alex ()
put s' = Alex $ \s -> Right (s{alex_ust = s'}, ())

modify :: (AlexUserState -> AlexUserState) -> Alex ()
modify f
  = Alex $ \s -> Right (s{alex_ust = f (alex_ust s)}, ())
\end{verbatim}
\subsection*{Analisadores léxicos}
\label{sec:org4a8189f}

\begin{itemize}
\item Transições entre estados:
\end{itemize}

\begin{verbatim}
-- multi-line comment
<0> "\*"              { nestComment `andBegin` state_comment }
<0> "*/"              {\ _ _ -> alexError "Error! Unexpected close comment!" }
<state_comment> "\*"  { nestComment }
<state_comment> "*/"  { unnestComment }
<state_comment> .     ;
<state_comment> \n    ;
\end{verbatim}
\section*{Conclusão}
\label{sec:org3a28047}

\subsection*{Conclusão}
\label{sec:orgaf90fbd}

\begin{itemize}
\item Análise léxica é responsável por decompor o código em \textbf{\textbf{tokens}}.
\item Eliminar comentários, espaços em branco do código.
\end{itemize}
\subsection*{Conclusão}
\label{sec:org43847c1}

\begin{itemize}
\item Análise léxica pode ser automatizada utilizando\ldots{}
\begin{itemize}
\item Expressões regulares e autômatos finitos.
\end{itemize}

\item No contexto de Haskell, podemos utilizar o gerador Alex.
\end{itemize}
\subsection*{Conclusão}
\label{sec:orgeb8fdf2}

\begin{itemize}
\item Existem geradores de analisadores léxicos para outras linguagens?
\begin{itemize}
\item Sim! O primeiro foi o \textbf{\textbf{lex}} para C.
\item Grande parte das linguagens possuem ferramentas similares.
\end{itemize}
\end{itemize}
\subsection*{Conclusão}
\label{sec:orgc4635cb}

\begin{itemize}
\item Vantagens de uso de geradores:
\begin{itemize}
\item \textbf{\textbf{Eficiência}}: código gerado é bastante eficiente.
\item \textbf{\textbf{Manutenção}}: fácil de incluir / remover tokens da linguagem.
\end{itemize}
\end{itemize}
\subsection*{Conclusão}
\label{sec:org23b40d9}

\begin{itemize}
\item Próxima aula: Análise sintática descendente recursiva.
\end{itemize}
\end{document}
